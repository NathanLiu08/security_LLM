The growing use of artificial intelligence has sparked major security worries about code produced by large language models (LLMs). This paper examines weaknesses in LLM-generated code, concentrating on the Llama 3 model. While these systems can speed up software development, our initial tests uncovered serious flaws—such as SQL injection, cross-site scripting (XSS), and buffer overflows—appearing across multiple programming languages. To address this, we fine-tuned Llama 3 with examples of secure coding practices and then built a simple “insecure code” detector aimed at spotting those specific issues. Findings show that fine-tuning lowered the frequency of some vulnerabilities but did not remove them entirely. The detector, however, was effective at catching remaining problems, especially when used alongside the fine-tuned model. Overall, the study underscores the need for more robust fine-tuning and stronger vulnerability detection, and calls for continued work on embedding security safeguards directly into LLMs to better protect AI-generated code.
